# Project Map: Document Management & Library

## document_library.py (1,501 lines)
- **Purpose:** Core document library â€” CRUD operations, thread storage, and semantic search/embeddings
- **Dependencies:** os, json, hashlib, datetime, config, utils
- **Called By:** Main.py, document_tree_manager.py, export_utilities.py, thread_viewer.py, smart_load.py
- **Data Store:** `document_library.json` at `LIBRARY_PATH`

### Library CRUD:
- `ensure_library()` â€” creates library file if absent
- `load_library()` â†’ Dict â€” reads library from disk
- `save_library(library)` â€” writes library to disk (atomic)
- `generate_doc_id(source, doc_type)` â†’ str â€” MD5-based 12-char hex ID
- `add_document_to_library(doc_type, source, title, entries, metadata, document_class)` â†’ str â€” add/update doc
- `update_document_entries(doc_id, new_entries)` â†’ bool
- `rename_document(doc_id, new_title)` â†’ bool
- `delete_document(doc_id)` â†’ bool
- `convert_document_to_source(doc_id)` â†’ bool â€” changes document_class

### Document Retrieval:
- `load_document_entries(doc_id)` â†’ List[Dict] or None â€” loads from summaries dir
- `get_recent_documents(limit)` â†’ List[Dict]
- `get_document_count()` â†’ int
- `get_document_by_id(doc_id)` â†’ Dict or None
- `get_all_documents()` â†’ List[Dict]
- `search_documents(query, search_in)` â†’ List[Dict]
- `get_library_stats()` â†’ Dict
- `is_source_document(doc_id)` â†’ bool

### Processed Outputs (AI analysis results):
- `add_processed_output_to_document(doc_id, prompt_name, prompt_text, output_text, model, provider, cost)` â†’ str
- `get_processed_outputs_for_document(doc_id)` â†’ List[Dict]
- `load_processed_output(output_id)` â†’ str or None
- `delete_processed_output(doc_id, output_id)` â†’ bool

### Thread/Conversation Storage:
- `save_thread_to_document(doc_id, thread, thread_metadata)` â†’ bool
- `load_thread_from_document(doc_id)` â†’ (thread, metadata)
- `clear_thread_from_document(doc_id)` â†’ bool
- `save_thread_as_new_document(original_doc_id, thread, metadata)` â†’ str â€” saves conversation as new library doc
- `format_thread_as_text(thread)` â†’ str
- `get_threads_for_document(doc_id)` â†’ list
- `get_response_branches_for_source(source_doc_id)` â†’ List[Dict]
- `delete_thread_document(thread_doc_id)` â†’ bool

### Semantic Search & Embeddings:
- `get_embeddings_path()` â†’ str
- `get_documents_needing_embeddings()` â†’ List[Dict]
- `get_embedding_stats()` â†’ Dict
- `perform_semantic_search(query, api_key, top_k, ...)` â†’ results
- `perform_semantic_search_all_chunks(query, api_key, top_k, ...)` â†’ results
- `generate_embedding_for_doc(doc_id, api_key, ...)` â†’ result
- `remove_embedding_for_doc(doc_id)` â†’ bool
- `has_embedding(doc_id)` â†’ bool
- `get_document_chunk_count(doc_id)` â†’ int

---

## document_tree_manager.py (1,470 lines)
- **Purpose:** Documents Library UI â€” tree-structured folder hierarchy with preview panel and editing
- **Pattern:** Built on `tree_manager_base.py` (TreeManager, TreeManagerUI)
- **Dependencies:** tkinter, tree_manager_base, document_library, document_export, config
- **Called By:** Main.py (via "Documents Library" button)

### Class: DocumentItem (extends TreeNode)
- Custom node for documents with `doc_id`, `doc_type`, `document_class`, `has_thread`, `metadata`
- `get_icon()` â†’ emoji based on type (ğŸ“„ğŸ“ŠğŸ“¹ğŸ™ï¸ etc.)
- `to_dict()` / `from_dict()` â€” serialization

### Class: DocumentTreeManagerUI (extends TreeManagerUI)
- 4-level folder hierarchy with drag-and-drop
- Preview panel (read-only for sources, editable for products)
- **Key Methods:**
  - `create_ui()` / `create_preview_panel()` â€” builds UI
  - `show_document_preview(doc)` â€” loads preview with metadata header
  - `enter_edit_mode()` / `exit_edit_mode(save)` â€” edit product documents
  - `load_document()` â€” opens doc in main window
  - `send_to_input()` â€” sends doc text to input panel
  - `perform_search()` / `highlight_search_results()` â€” library search
  - `delete_selected()` â€” handles single/multi/folder deletion
  - `rename_selected()` â€” inline rename
  - `save_tree()` â€” persists folder structure

### Module-Level Functions:
- `load_document_tree(library_path)` â†’ TreeManager â€” loads/creates tree from JSON
- `sync_tree_with_library(tree, library_path)` â€” syncs tree with library (adds new, removes deleted)
- `open_document_tree_manager(parent, library_path, ...)` â€” main entry point, opens modal dialog

---

## document_fetcher.py (1,059 lines)
- **Purpose:** Low-level document fetching â€” reads local files (PDF, DOCX, TXT, RTF, spreadsheets) and web URLs
- **Dependencies:** os, re, tempfile, docx, PyPDF2, bs4, requests, openpyxl, csv, pydub
- **Called By:** document_fetching.py, ocr_processing.py, smart_load.py

### Text Processing:
- `clean_text_encoding(text)` â†’ str â€” fixes mojibake/encoding artifacts
- `legacy_entries_to_text(entries, include_timestamps)` â†’ str â€” converts entry list to plain text

### Local File Handling:
- `fetch_local_file(filepath)` â†’ (success, entries/text, title, doc_type) â€” dispatches by extension:
  - PDF â†’ PyPDF2 text extraction (with OCR pre-screening)
  - DOCX â†’ python-docx paragraph extraction
  - TXT/MD/RTF â†’ direct read
  - XLSX/CSV/TSV â†’ spreadsheet â†’ text
  - Audio/video â†’ returns file path for transcription
  - Images â†’ returns path for OCR

### Web URL Handling:
- `fetch_web_url(url)` â†’ (success, entries, title, doc_type, metadata) â€” BeautifulSoup scraping
- `extract_publication_date(soup, html_text)` â†’ str â€” extracts date from meta tags/text
- `fetch_web_video(url, api_key, engine, options, ...)` â€” downloads and transcribes web video

### Google Sheets Support:
- `is_google_sheets_url(url)` â†’ bool
- `extract_google_sheets_id(url)` â†’ str or None
- `download_google_sheet(url, sheet_id)` â†’ (success, csv_content, title)
- `fetch_google_sheet(url)` â†’ (success, entries, title, doc_type)

---

## document_fetching.py (1,855 lines)
- **Purpose:** High-level document fetching orchestration â€” **Mixin class** extracted from Main.py
- **Pattern:** `DocumentFetchingMixin` â€” mixed into main app class, uses `self.xxx`
- **Dependencies:** document_fetcher, substack_utils, twitter_utils, video_platform_utils, youtube_utils
- **Called By:** Main.py (mixed in via inheritance)

### YouTube:
- `fetch_youtube()` â€” validates URL, starts threaded fetch
- `_fetch_youtube_thread()` â€” calls youtube_utils for transcript/audio
- `_handle_youtube_result(success, result, title, source_type, yt_metadata)` â€” displays result, auto-saves

### Substack:
- `fetch_substack()` â€” validates URL, starts threaded fetch
- `_fetch_substack_thread()` â€” fetches article text and/or podcast audio
- `_ask_substack_content_choice_simple(choice_data)` â€” dialog: text, audio, or both
- `_display_substack_result(text, title, metadata)` â€” loads into app

### Twitter/X:
- `fetch_twitter(url)` â€” starts threaded fetch
- `_show_twitter_content_choice(result, title, url)` â€” dialog: text, video, or both
- `_download_and_transcribe_twitter(url, title)` â€” video transcription path

### Video Platforms (Vimeo, Dailymotion, etc.):
- `fetch_video_platform(url)` â€” dispatches to video_platform_utils
- `_handle_video_platform_result(...)` â€” loads transcript

### Local Files:
- `browse_file()` â€” file dialog, delegates to `_fetch_local_file_thread()`
- `_fetch_local_file_thread()` â€” threaded file processing with OCR detection
- `_handle_file_result(success, result, title)` â€” loads result
- `_handle_spreadsheet_result(text_content, title, file_path)` â€” spreadsheet handling

### Other:
- `start_dictation()` â€” opens dictation dialog
- `open_multi_image_ocr()` â€” opens multi-image OCR dialog
- `_show_paste_fallback_dialog(url, source_type, source_name)` â€” paste content when fetch fails

---

## smart_load.py (1,298 lines)
- **Purpose:** Smart loading, URL auto-detection, multi-document handling â€” **Mixin class**
- **Pattern:** `SmartLoadMixin` â€” mixed into main app class
- **Dependencies:** document_fetcher, ocr_handler, youtube_utils, video_platform_utils
- **Called By:** Main.py (mixed in via inheritance)

### Smart Load (main entry point):
- `smart_load()` â€” auto-detects input type (YouTube URL/ID, Substack, Twitter, Google Drive, web URL, file path, multi-line input)
- `process_url_or_id()` â€” alias for smart_load

### Multi-Document Handling:
- `_process_multiple_inputs(input_lines)` â€” routes multiple inputs
- `_show_multi_document_dialog(input_lines)` â€” dialog: process individually or combine
- `_combine_documents_for_analysis(input_lines, analysis_name)` â€” combines multiple docs
- `_load_combined_documents(input_lines, analysis_name)` â€” fetches each doc
- `_finalize_combined_documents(loaded_documents, analysis_name)` â€” merges into one
- `_batch_process_inputs(input_lines)` â€” batch process mode

### URL Detection:
- `is_youtube_url(url)` â†’ bool
- `is_substack_url(url)` â†’ bool
- `_is_google_drive_file_url(url)` / `_is_google_drive_folder_url(url)` â†’ bool
- `_extract_google_drive_file_id(url)` â†’ str
- `_fetch_google_drive_file(url)` â€” downloads from Google Drive
- `could_be_youtube_id(text)` â†’ bool

### OCR Detection:
- `_is_image_file(filepath)` â†’ bool
- `_needs_ocr(filepath)` â†’ bool
- `_check_ocr_confidence(image_path)` â†’ confidence score
- `_process_images_standard_ocr(ocr_files, combine)` â€” OCR for multiple images

---

## save_utils.py (385 lines)
- **Purpose:** Universal save/export functions with consistent metadata formatting
- **Dependencies:** os, datetime, docx, reportlab
- **Called By:** Main.py, thread_viewer.py, process_output.py

### Key Functions:
- `save_document_to_file(filepath, format, content, title, source, ...)` â†’ (bool, str) â€” exports to TXT/DOCX/RTF/PDF
- `get_clean_filename(title, max_length)` â†’ str â€” sanitizes for filesystem
- `get_document_metadata(app_instance, get_document_by_id_func)` â†’ dict â€” extracts current doc metadata
- `prompt_and_save_document(parent, content, title, metadata)` â€” file dialog + save

---

## universal_document_saver.py (455 lines)
- **Purpose:** Auto-save framework â€” saves all incoming documents to library automatically
- **Dependencies:** document_library, config
- **Called By:** Main.py (wraps all fetch handlers)

### Class: UniversalDocumentSaver
- `save_source_document(entries, title, doc_type, source, metadata, document_class)` â†’ str â€” saves to library
- `enable()` / `disable()` / `toggle()` â€” control auto-save
- `get_last_saved_id()` â†’ str or None

### Legacy Handler Stubs (for backward compatibility):
- `handle_youtube_url(url)`, `handle_pdf_file(filepath)`, `handle_ocr_scan(image_paths)`
- `handle_audio_file(audio_path)`, `handle_dictation(audio_data)`, `handle_web_url(url)`

---

## document_export.py (844 lines)
- **Purpose:** Consolidated export for documents AND conversation threads to TXT/DOCX/RTF/PDF
- **Dependencies:** os, re, datetime, docx, reportlab
- **Called By:** thread_viewer.py, document_tree_manager.py, export_utilities.py

### Document Export:
- `export_document(format, content, title, source, ...)` â†’ (bool, str, filepath) â€” main export
- `_export_as_txt/docx/rtf/pdf(...)` â€” format-specific implementations

### Thread Export:
- `export_conversation_thread(format, messages, metadata, ...)` â†’ (bool, str, filepath)
- `_export_thread_as_txt/docx/rtf/pdf(...)` â€” format-specific thread exports

### Formatting Helpers:
- `sanitize_filename(text, max_length)` â†’ str
- `_escape_pdf_text(text)` â†’ str
- `_markdown_to_pdf_html(text)` â†’ str
- `_add_markdown_content_to_docx(doc, content)` â€” rich markdown â†’ DOCX
- `_add_inline_markdown_to_paragraph(paragraph, text)` â€” bold/italic/code in DOCX

---

## doc_formatter.py (637 lines)
- **Purpose:** Enhanced formatting for AI responses with markdown support in all export formats
- **Dependencies:** docx, reportlab
- **Called By:** save_utils.py, document_export.py

### Key Functions:
- `parse_markdown_text(text)` â†’ list â€” parses markdown into structured elements
- `add_formatted_paragraph(doc, text, style, is_italic)` â€” adds to DOCX with formatting
- `save_formatted_docx(filepath, content, title, ...)` â†’ (bool, str) â€” full DOCX with headers/bullets/tables
- `save_formatted_txt(filepath, content, title, ...)` â†’ (bool, str)
- `save_formatted_pdf(filepath, content, title, ...)` â†’ (bool, str)
- `save_formatted_document(filepath, format, content, ...)` â€” dispatcher

---

## output_formatter.py (245 lines)
- **Purpose:** RTF generation and markdown rendering in Tkinter text widgets
- **Dependencies:** tkinter
- **Called By:** Main.py, thread_viewer.py

### Key Functions:
- `generate_rtf_content(title, content, metadata)` â†’ str â€” RTF string with formatting
- `render_markdown_in_text_widget(text_widget, content, font_size)` â€” renders **bold**, *italic*, `code`, headers, bullets in Tkinter
- `generate_html_content(title, content, metadata)` â†’ str
- `generate_markdown_content(title, content, metadata)` â†’ str

---

## process_output.py (927 lines)
- **Purpose:** AI processing orchestration and output saving â€” **Mixin class**
- **Pattern:** `ProcessOutputMixin` â€” mixed into main app class
- **Dependencies:** ai_handler, document_library, output_formatter, doc_formatter
- **Called By:** Main.py (mixed in via inheritance)

### Processing:
- `process_document()` â€” validates inputs, starts AI processing thread
- `_process_document_thread()` â€” sends chunks to AI, handles streaming, manages attachments
- `check_processing_thread()` â€” polls processing thread status
- `cancel_processing()` / `_show_cancel_confirmation()` â€” cancellation flow
- `_restart_application()` â€” force restart after stuck processing

### Output Handling:
- `_handle_process_result(success, result)` â€” displays AI response
- `_save_processed_output(ai_response)` â€” saves to library as processed output
- `_save_attachments_output(ai_response)` â€” saves with attachment context
- `_save_as_product(output_text, dialog)` â€” saves as product document
- `_save_as_metadata(output_text, dialog)` â€” saves as document metadata
- `save_current_output(output_text)` / `save_output()` â€” user-triggered save

### UI State:
- `reset_ui_state()` â€” resets buttons/progress after processing

---

## export_utilities.py (864 lines)
- **Purpose:** App lifecycle, export, and utility methods â€” **Mixin class**
- **Pattern:** `ExportUtilitiesMixin` â€” mixed into main app class
- **Dependencies:** document_library, document_export, cost_tracker, semantic_search, sources_dialog
- **Called By:** Main.py (mixed in via inheritance)

### App Lifecycle:
- `on_app_closing()` â€” cleanup and exit
- `start_new_conversation_same_source(source_doc_id)` â†’ bool â€” creates new conversation branch

### Export:
- `export_to_web_chat()` â€” exports thread as shareable web page
- `export_document()` â€” file export dialog
- `save_thread_to_library()` â€” saves current conversation to library

### External Tools:
- `send_to_turboscribe()` â€” sends audio to TurboScribe for transcription
- `import_turboscribe()` â€” imports TurboScribe results
- `force_reprocess_pdf()` â€” re-extracts PDF text
- `download_youtube_video()` â€” downloads video file
- `open_web_url_in_browser()` â€” opens source URL

### Other:
- `test_semantic_search()` â€” runs semantic search dialog
- `show_costs()` â€” opens cost tracker
- `open_add_sources()` / `update_add_sources_button()` â€” multi-source attachment dialog
